Technical Design Document

Adobe Experience Manager (AEM) with Imperva WAF + CDN, NGINX Redirection Layer, and S3 as CDN Storage

‚∏ª

1. Scope & Objectives

1.1 Scope

This document covers:
	‚Ä¢	Public website delivery using Adobe Experience Manager (AEM).
	‚Ä¢	Imperva as:
	‚Ä¢	External WAF (Web Application Firewall)
	‚Ä¢	CDN cache / edge delivery
	‚Ä¢	Origin routing (AEM vs S3)
	‚Ä¢	NGINX as an internal reverse proxy + redirection and URL management layer in front of AEM publish.
	‚Ä¢	AWS S3 as a static asset origin (CDN storage) configured on Imperva side.

1.2 Objectives
	‚Ä¢	Provide secure, high-performance delivery of AEM-hosted sites.
	‚Ä¢	Offload as much as possible to Imperva edge cache + S3.
	‚Ä¢	Centralize security & HTTP policy in Imperva/NGINX while keeping AEM simple.
	‚Ä¢	Standardize URL patterns, redirects, cache headers, and logging.

‚∏ª

2. High-Level Architecture
Client (Browser / App)
        |
        v
   Public DNS
  (www.example.com)
        |
        v
   Imperva WAF + CDN
        |     \
        |      \
        |       +--> Origin 2: AWS S3 (static content, images, assets)
        v
Origin 1: NGINX (reverse proxy / redirect layer)
        |
        v
   AEM Publish Tier
 (1..n AEM publish instances)



2.2 Origins
	‚Ä¢	Origin 1: NGINX
	‚Ä¢	Connects to AEM Publish over internal network.
	‚Ä¢	Handles:
	‚Ä¢	Legacy URL redirects (301/302)
	‚Ä¢	HTTP ‚Üí HTTPS redirection (if needed internally)
	‚Ä¢	Canonical host redirects (e.g., example.com ‚Üí www.example.com)
	‚Ä¢	Basic header normalization.
	‚Ä¢	Origin 2: S3 Bucket
	‚Ä¢	Holds static assets:
	‚Ä¢	Versioned static files (/static/*)
	‚Ä¢	Heavy media / images if needed.
	‚Ä¢	Configured as a separate HTTP(S) origin in Imperva.

‚∏ª

3. DNS & Network Topology

3.1 DNS
	‚Ä¢	www.example.com
‚Üí CNAME ‚Üí <imperva-edge-alias>.impervadns.net (example placeholder)

(Optional)
	‚Ä¢	static.example.com
‚Üí CNAME ‚Üí <imperva-static-edge>.impervadns.net
(Mapped to S3 origin via Imperva)

3.2 Network Zones
	‚Ä¢	Public zone: Client ‚Üí Imperva (Internet-facing).
	‚Ä¢	DMZ / Edge zone: Imperva ‚Üí NGINX ‚Üí AEM (over secure network).
	‚Ä¢	Private zone: AEM Publish instances (no direct Internet exposure).
	‚Ä¢	AWS zone: S3 bucket (public-only-via-Imperva or private with OAI / signed origin access).

‚∏ª

4. Imperva Configuration Details

4.1 Imperva Site Configuration
	‚Ä¢	Site / Application in Imperva portal
	‚Ä¢	Primary hostname: www.example.com
	‚Ä¢	Additional hostnames: example.com, static.example.com (if used).
	‚Ä¢	Protocols: HTTPS only (redirect HTTP ‚Üí HTTPS at Imperva).

4.2 Origins
	1.	Origin: NGINX
	‚Ä¢	Type: Web Server
	‚Ä¢	Host: nginx.internal.example.com (or IP)
	‚Ä¢	Port: 443 (preferred) or 80
	‚Ä¢	TLS:
	‚Ä¢	If TLS: set Origin Certificate (internal CA or self-signed; configure Imperva to trust it).
	‚Ä¢	Health checks:
	‚Ä¢	Path: /healthcheck.html or /system/health
	‚Ä¢	Interval: 30s‚Äì60s, 2‚Äì3 retries.
	2.	Origin: AWS S3
	‚Ä¢	Type: S3 / Object Storage
	‚Ä¢	Host: <bucket-name>.s3.<region>.amazonaws.com
	‚Ä¢	Access:
	‚Ä¢	Either public read-only bucket (with IP whitelist to Imperva)
	‚Ä¢	Or private bucket with:
	‚Ä¢	Bucket policy allowing only Imperva‚Äôs IP ranges
	‚Ä¢	Or signed origin access mechanism if supported in your setup.
	‚Ä¢	Health check:
	‚Ä¢	Path /health.txt or a lightweight asset.

4.3 Routing Rules (Imperva)

Example rule-set:
	1.	Static Assets via S3
	‚Ä¢	Condition: URL path starts with /static/ OR host is static.example.com
	‚Ä¢	Action: Route to Origin ‚ÄúS3-static-origin‚Äù
	‚Ä¢	Caching:
	‚Ä¢	Cache-Control: public, max-age=31536000 (1 year for versioned files)
	‚Ä¢	Use Query string and path as cache key.
	2.	AEM Dynamic Content via NGINX
	‚Ä¢	Condition: default (all unmatched)
	‚Ä¢	Action: Route to Origin ‚Äúnginx-aem-origin‚Äù
	‚Ä¢	Caching:
	‚Ä¢	Respect origin headers from NGINX/AEM.
	‚Ä¢	For HTML: typically short TTL (30s‚Äì300s) or ‚Äúno-store‚Äù for highly dynamic pages.
	3.	Legacy paths to be redirected at Imperva (optional)
	‚Ä¢	Rules to return HTTP 301 for specific patterns:
	‚Ä¢	/old-page ‚Üí /new-page
	‚Ä¢	http:// ‚Üí https://
	‚Ä¢	This avoids unnecessary hop to NGINX for simple redirects.

4.4 WAF Security Policies
	‚Ä¢	Core rules:
	‚Ä¢	OWASP Top 10 signatures (SQLi, XSS, RCE, etc.).
	‚Ä¢	Protocol enforcement: disallow invalid methods (TRACE, TRACK, etc.).
	‚Ä¢	Geo-blocking / throttling if needed.
	‚Ä¢	Custom rules:
	‚Ä¢	Whitelist known legitimate integrations (monitoring, search crawlers).
	‚Ä¢	Block specific patterns: e.g., /system/console (AEM admin), etc.
	‚Ä¢	Rate Limiting / Bot Mitigation:
	‚Ä¢	Enable bot classification & challenge.
	‚Ä¢	Set per-IP and per-path thresholds.

4.5 TLS & Certificates
	‚Ä¢	Imperva terminates TLS:
	‚Ä¢	Public certificate for www.example.com (managed in Imperva).
	‚Ä¢	Imperva ‚Üí NGINX:
	‚Ä¢	Prefer HTTPS:
	‚Ä¢	NGINX uses internal certificate (self-signed / internal CA).
	‚Ä¢	Imperva configured to validate or ignore depending on your security posture.

‚∏ª

5. NGINX Configuration Details

NGINX acts as reverse proxy + redirect layer between Imperva and AEM.

5.1 NGINX High-Level Responsibilities
	‚Ä¢	Reverse proxy to AEM publish.
	‚Ä¢	Enforce basic security headers.
	‚Ä¢	Handle:
	‚Ä¢	Canonical domain redirection (if traffic hits internal hostnames).
	‚Ä¢	Custom URL rewrites (if not done at Imperva).
	‚Ä¢	Optional: serve simple health-check pages.

5.2 Sample NGINX Config (Simplified)


6. AEM Publish & Caching Strategy

6.1 AEM Publish Tier
	‚Ä¢	2+ AEM publish instances behind NGINX upstream.
	‚Ä¢	Each instance:
	‚Ä¢	Hosts the site content under /content/<site> and /etc.clientlibs/... (if using clientlibs).
	‚Ä¢	Has only internal access (no public exposure).

6.2 Cache-Control Strategy

You want AEM ‚Üí NGINX ‚Üí Imperva to respect a coherent set of cache headers:
	1.	HTML Pages
	‚Ä¢	Cache-Control: max-age=60, public or no-cache depending on freshness requirements.
	‚Ä¢	Often short TTL + Imperva provides global edge caching.
	2.	Clientlibs / Static JS/CSS (from AEM or S3)
	‚Ä¢	Use fingerprinted filenames (hash in filename).
	‚Ä¢	Cache-Control: public, max-age=31536000, immutable.
	3.	Personalized or authenticated content
	‚Ä¢	Cache-Control: no-store, private and Vary: Cookie.

6.3 AEM Dispatcher (Optional)
Imperva -> NGINX -> Apache HTTPD + Dispatcher -> AEM Publish


‚Ä¢	NGINX points to Apache dispatcher instead of direct publish.
	‚Ä¢	Dispatcher handles caching for HTML and clientlibs; Imperva then caches that output again at edge.
	‚Ä¢	This can give you 2-layer caching (dispatcher+Imperva) but adds complexity.

‚∏ª

7. S3 Bucket Design (CDN Storage)

7.1 Bucket Structure

Bucket: aem-static-prod-example

7.2 Access & Security
	‚Ä¢	S3 bucket made non-public if possible:
	‚Ä¢	Bucket policy: allow s3:GetObject only from Imperva‚Äôs IPs / IAM role.
	‚Ä¢	Objects have:
	‚Ä¢	Content-Type properly set (text/css, application/javascript, image/png, etc.).
	‚Ä¢	Cache-Control header (long TTLs).

7.3 CI/CD for Static Assets
	‚Ä¢	Build pipeline:
	1.	Frontend build (webpack/rollup) ‚Üí produce /dist folder.
	2.	Upload /dist to s3://aem-static-prod-example/static/vX/.
	3.	Invalidate Imperva cache for selected paths:
	‚Ä¢	/static/vX/* or /static/* depending on strategy.
	4.	Update AEM configs / ClientLibs to point to new vX.

‚∏ª

8. Request Flows

8.1 HTML Page Request
	1.	Client requests https://www.example.com/home.
	2.	DNS resolves www.example.com ‚Üí Imperva.
	3.	Imperva:
	‚Ä¢	Runs WAF checks.
	‚Ä¢	Cache lookup:
	‚Ä¢	If HIT, return from edge.
	‚Ä¢	If MISS, forward to origin NGINX.
	4.	NGINX:
	‚Ä¢	Proxies to aem_publish upstream.
	5.	AEM Publish:
	‚Ä¢	Generates HTML.
	‚Ä¢	Sets headers (Cache-Control, etc.).
	6.	NGINX returns response to Imperva.
	7.	Imperva:
	‚Ä¢	Caches response per policy.
	‚Ä¢	Returns to client.

8.2 Static Asset Request (S3)
	1.	Client requests https://www.example.com/static/v1/js/main.js
(or https://static.example.com/static/v1/js/main.js).
	2.	Imperva:
	‚Ä¢	WAF checks.
	‚Ä¢	Route based on path/host to S3 origin.
	3.	S3 returns asset.
	4.	Imperva caches long-term at edge.
	5.	Response returns to client.

‚∏ª

9. Security Considerations

9.1 AEM Attack Surface
	‚Ä¢	AEM author is not exposed via Imperva.
	‚Ä¢	Only publish instances accessible (and only via NGINX).
	‚Ä¢	Ensure paths like:
	‚Ä¢	/system/console
	‚Ä¢	/crx/de
	‚Ä¢	/libs/cq/core/content/login.html
are blocked or protected at:
	‚Ä¢	Imperva level (WAF rule) and/or
	‚Ä¢	NGINX level (return 403).

9.2 Security Headers

Applied at NGINX or AEM:
	‚Ä¢	Strict-Transport-Security: max-age=31536000; includeSubDomains
	‚Ä¢	Content-Security-Policy (CSP) as per app requirements.
	‚Ä¢	X-Frame-Options, X-Content-Type-Options, Referrer-Policy.

9.3 Logging & Monitoring
	‚Ä¢	Imperva logs:
	‚Ä¢	WAF events, blocked requests.
	‚Ä¢	Edge cache stats, origin errors.
	‚Ä¢	NGINX access/error logs:
	‚Ä¢	Correlate with Imperva logs using request IDs (e.g., add header X-Request-ID at Imperva, log it in NGINX).
	‚Ä¢	AEM logs:
	‚Ä¢	Standard request.log + error.log.

‚∏ª

10. Non-Functional Requirements

10.1 Performance
	‚Ä¢	Imperva edge caching for:
	‚Ä¢	Static assets: TTL ‚â• 1 day (ideally 1 year for versioned).
	‚Ä¢	HTML content: small but non-zero TTL where possible.
	‚Ä¢	NGINX tuned for:
	‚Ä¢	Keepalive with AEM publish.
	‚Ä¢	Reasonable buffer sizes and timeouts.

10.2 High Availability
	‚Ä¢	AEM publish: 2+ nodes.
	‚Ä¢	NGINX: 2+ instances behind internal load balancer or VIP.
	‚Ä¢	Imperva: managed HA by provider.

10.3 DR / Failover
	‚Ä¢	Option 1: Backup S3 bucket and DR AEM environment in another region.
	‚Ä¢	Option 2: Imperva routing rules to switch origin to DR NGINX/AEM.

‚∏ª

11. Open Points / Decisions

You can adapt this section for your org:
	1.	Use AEM dispatcher or direct publish behind NGINX?
	2.	Which specific Imperva WAF policy set for this app?
	3.	Do we use a dedicated host static.example.com for S3, or same host under /static/?
	4.	Logging pipeline: ship Imperva, NGINX, and AEM logs to SIEM?

‚∏ª

If you tell me:
	‚Ä¢	Your actual domains,
	‚Ä¢	Whether you‚Äôre using dispatcher,
	‚Ä¢	And if you prefer single domain or separate static domain,

I can tailor this into a final, ready-to-send PDF-style spec with concrete hostnames and exact example rules.

Technical Design ‚Äì Introducing NGINX Proxy Tier When Imperva Rules Are Saturated for sc.com

1. Context & Problem Statement

1.1 Existing Situation
	‚Ä¢	The public banking domain sc.com (and subdomains) is fronted by Imperva.
	‚Ä¢	Imperva currently handles:
	‚Ä¢	WAF (ruleset is already complex, centrally managed by Security)
	‚Ä¢	L3/L4 & L7 DDoS protection
	‚Ä¢	CDN caching and TLS termination
	‚Ä¢	Over time, Imperva rules for sc.com have become:
	‚Ä¢	Highly complex (large number of URL-based rules, rewrites, redirects).
	‚Ä¢	Change-controlled and tightly governed by central security.
	‚Ä¢	Hard/slow to maintain for application-specific routing and redirection.

1.2 Problem

For new applications / microsites under sc.com (e.g., AEM sites, campaign pages), teams need:
	‚Ä¢	New redirects and URL rewrites (e.g., /old-product ‚Üí /new-product).
	‚Ä¢	Application-specific routing logic (e.g., /aem/* ‚Üí AEM, /legacy/* ‚Üí legacy app).
	‚Ä¢	Fast change cycles (per sprint, even per release).

However:
	‚Ä¢	Adding every redirect / rewrite / routing rule directly into Imperva is no longer feasible:
	‚Ä¢	Governance: security team does not want Imperva to be treated as an app router.
	‚Ä¢	Complexity: risk of misconfigurations (rules collide, impact other paths/apps).
	‚Ä¢	Operational overhead: every small redirect becomes a WAF change ticket.

Conclusion: We need an application-facing proxy layer (NGINX / NGINX Plus) behind Imperva, so Imperva stays focused on security and edge caching, and all app routing & redirection logic moves to a dedicated, dev-owned proxy.

‚∏ª

2. Target Responsibilities Split

2.1 Imperva Responsibilities (stay focused)
	‚Ä¢	Security perimeter & reliability
	‚Ä¢	WAF (OWASP Top 10 protection, custom signatures).
	‚Ä¢	DDoS protection.
	‚Ä¢	Bot mitigation and IP reputation filtering.
	‚Ä¢	TLS termination & CDN
	‚Ä¢	Public certificates for sc.com & subdomains.
	‚Ä¢	Edge caching for static content (depending on headers).
	‚Ä¢	Minimal HTTP logic
	‚Ä¢	HTTP ‚Üí HTTPS redirect (global).
	‚Ä¢	Basic host ‚Üí origin mapping (e.g., www.sc.com ‚Üí proxy origin).
	‚Ä¢	Very limited, generic rules only where absolutely necessary.

2.2 New NGINX / NGINX Plus Responsibilities
	‚Ä¢	Application routing
	‚Ä¢	Path-based routing for AEM / legacy / microservices:
	‚Ä¢	/aem/* ‚Üí AEM publish
	‚Ä¢	/legacy/* ‚Üí legacy Java app
	‚Ä¢	/api/* ‚Üí backend APIs
	‚Ä¢	Redirections
	‚Ä¢	All new business redirects:
	‚Ä¢	Legacy URLs ‚Üí new URLs.
	‚Ä¢	Campaign URLs ‚Üí deep links.
	‚Ä¢	SEO redirects (301) and temporary redirects (302).
	‚Ä¢	URL Rewrites & Normalization
	‚Ä¢	Trailing slash normalization.
	‚Ä¢	Case-insensitive redirects (if needed).
	‚Ä¢	Country / language routing, if required.
	‚Ä¢	Header normalization
	‚Ä¢	Add/remove headers for backend services.
	‚Ä¢	Inject correlation IDs for observability.

Imperva remains ‚Äúclean‚Äù and generic; the proxy becomes the app team‚Äôs programmable edge.

Client (Browser / App)
        |
        v
   Public DNS
  (www.sc.com)
        |
        v
   Imperva (WAF + CDN + DDoS)
        |
        |  (single or minimal origin mapping for www.sc.com)
        v
   NGINX / NGINX Plus (Proxy Tier)
        |
        +--> AEM Publish Cluster
        |
        +--> Legacy Web / Portal
        |
        +--> Backend APIs / Microservices


DNS www.sc.com ‚Üí Imperva.
	‚Ä¢	Imperva forwards all traffic for www.sc.com to a single origin: the NGINX/NGINX Plus proxy.
	‚Ä¢	NGINX owns all path-based routing and redirects.

‚∏ª

4. Detailed Flow

4.1 Example: New AEM Page with Redirect

Scenario: /personalbanking/cards is moved to /personal/cards.

Request flow:
	1.	Client requests: https://www.sc.com/personalbanking/cards
	2.	DNS: www.sc.com ‚Üí Imperva.
	3.	Imperva:
	‚Ä¢	Runs WAF checks & DDoS filters.
	‚Ä¢	Checks CDN cache:
	‚Ä¢	If response not cached, forwards to NGINX origin (proxy.sc.internal).
	4.	NGINX:
	‚Ä¢	Matches redirect map:
	‚Ä¢	/personalbanking/cards ‚Üí /personal/cards (301).
	‚Ä¢	Returns redirect response.
	5.	Imperva:
	‚Ä¢	Can cache this 301 (short TTL) to reduce load.
	6.	Client follows redirect to https://www.sc.com/personal/cards (served via AEM or another backend through NGINX).

No new Imperva rule needed; change is done on the proxy layer, which is in the app team‚Äôs control.

‚∏ª

5. NGINX / NGINX Plus Design

5.1 Deployment & HA
	‚Ä¢	At least two NGINX / NGINX Plus instances in an internal DMZ.
	‚Ä¢	Exposed to Imperva via an internal VIP / Load Balancer:
	‚Ä¢	proxy.sc.internal:443
	‚Ä¢	All backend applications reachable from this proxy tier in a secure subnet.

5.2 Sample NGINX Config ‚Äì Routing & Redirects

Note: this focuses on the ‚Äúrules moved out of Imperva‚Äù aspect.


5.3 NGINX Plus Extras (if used)

If you opt for NGINX Plus, you gain:
	‚Ä¢	Dynamic reconfiguration of upstreams via API (no restart).
	‚Ä¢	Key-value stores for large redirect tables (thousands of rules) without recompiling map.
	‚Ä¢	Advanced health checks and active monitoring.
	‚Ä¢	Better observability (per-URI metrics, status API).

This is useful if you expect hundreds or thousands of redirects for sc.com.

‚∏ª

6. Imperva Configuration in This Model

6.1 Imperva Site for www.sc.com
	‚Ä¢	Origin: proxy.sc.internal (VIP of NGINX tier).
	‚Ä¢	WAF Policy: existing sc.com policy retained.
	‚Ä¢	Caching:
	‚Ä¢	Honor cache headers from NGINX/backend.
	‚Ä¢	Allow caching of 301 responses for shorter TTL (e.g., 5‚Äì30 minutes).

6.2 Rule Simplification

What we remove from Imperva:
	‚Ä¢	Path-based routing rules (e.g., /legacy/ ‚Üí legacy origin).
	‚Ä¢	Most business redirects (e.g., /old-url ‚Üí /new-url).
	‚Ä¢	Complex country / campaign specific rewrites.

What we keep in Imperva:
	‚Ä¢	Global HTTP ‚Üí HTTPS redirect (if not already done by load balancer).
	‚Ä¢	Basic host ‚Üí origin mapping: www.sc.com ‚Üí proxy origin.
	‚Ä¢	Security rules: WAF signatures, IP blacklist/whitelist, DDoS settings.

This significantly reduces rule explosion in Imperva for sc.com.

‚∏ª

7. Change & Governance Model

7.1 Before
	‚Ä¢	Every redirect / rewrite required:
	‚Ä¢	Imperva change ticket.
	‚Ä¢	Security review.
	‚Ä¢	Longer lead time.
	‚Ä¢	Imperva rule base became the place where business routing logic lived.

7.2 After
	‚Ä¢	Imperva:
	‚Ä¢	Only security team changes (WAF policies, DDoS settings).
	‚Ä¢	NGINX:
	‚Ä¢	Owned by application / platform team.
	‚Ä¢	Redirect/routing changes can be:
	‚Ä¢	Version-controlled (Git).
	‚Ä¢	Included with application release pipelines.
	‚Ä¢	Tested in lower environments with the same proxy setup.

Result:
Security team keeps control of perimeter; app team gets agility for URL behavior.

‚∏ª

8. Migration Strategy
	1.	Introduce NGINX in non-prod behind Imperva:
	‚Ä¢	Mirror the Imperva path/routing rules in NGINX config.
	2.	Move routing/redirect logic:
	‚Ä¢	Gradually remove non-security Imperva rules once equivalent NGINX rules are in place.
	3.	Cutover:
	‚Ä¢	Update Imperva origin for www.sc.com to point to NGINX VIP.
	4.	Decommission old Imperva site rules:
	‚Ä¢	Keep only minimal host mapping and security rules.
	5.	Standardize pattern for new apps:
	‚Ä¢	Any new app under www.sc.com is onboarded via NGINX routing, not via Imperva rule changes.

‚∏ª

9. Summary

Because Imperva rules for sc.com are already very complex and centrally governed, pushing more app-specific routing and redirect logic into Imperva is no longer scalable.

By introducing a proxy tier (NGINX / NGINX Plus) behind Imperva:
	‚Ä¢	Imperva remains focused on WAF + cache + DDoS.
	‚Ä¢	The proxy layer owns all URL behavior (routing, redirects, rewrites).
	‚Ä¢	App teams get faster change cycles using config-as-code.
	‚Ä¢	Security team retains a clean, manageable Imperva configuration for sc.com.

‚∏ª

If you want, I can now merge both scenarios into one unified document:
	1.	AEM + Imperva + NGINX + S3 (from earlier), and
	2.	This ‚ÄúImperva rule saturation ‚Üí proxy tier‚Äù justification,

‚Ä¶so you can present it as a single architecture proposal for sc.com.

There are quite a few, and they‚Äôre not just ‚Äútechnical‚Äù ‚Äì a lot are process / ownership issues. I‚Äôll break them down so you can reuse this as a ‚ÄúRisks & Challenges‚Äù section.

‚∏ª

1. More Moving Parts = More Complexity

Challenge:
You‚Äôre adding another hop: Client ‚Üí Imperva ‚Üí NGINX ‚Üí AEM/Legacy/API/S3.
	‚Ä¢	More components to design, deploy, monitor, patch.
	‚Ä¢	More places where headers, timeouts, and TLS settings must be consistent.
	‚Ä¢	Debugging becomes multi-layer:
	‚Ä¢	‚ÄúIs it Imperva?‚Äù
	‚Ä¢	‚ÄúIs it NGINX?‚Äù
	‚Ä¢	‚ÄúIs it AEM / backend?‚Äù

Mitigation ideas:
	‚Ä¢	Standardize proxy patterns (shared proxy_params, timeout/circuit-breaker defaults).
	‚Ä¢	Enforce a trace ID (X-Request-ID) from Imperva ‚Üí NGINX ‚Üí backend and log it everywhere.
	‚Ä¢	Have a documented runbook: ‚ÄúHow to debug a 5xx for sc.com‚Äù.

‚∏ª

2. Ownership & Change Management

Challenge:
Previously, everything happened in Imperva. Now you‚Äôre splitting responsibilities:
	‚Ä¢	Imperva changes ‚Üí Security team.
	‚Ä¢	NGINX routing/redirects ‚Üí App / Platform team.
	‚Ä¢	AEM content and S3 assets ‚Üí Content / Dev / DevOps teams.

This can create:
	‚Ä¢	‚ÄúWho owns this redirect?‚Äù confusion.
	‚Ä¢	Parallel change processes (CAB for Imperva, different flow for NGINX).
	‚Ä¢	Risk of people still requesting Imperva changes out of habit.

Mitigation ideas:
	‚Ä¢	Write a clear RACI:
	‚Ä¢	Imperva: WAF, DDoS, global HTTPS, minimal origin mapping.
	‚Ä¢	NGINX: all routing, redirects, rewrites for sc.com.
	‚Ä¢	Introduce a single intake for URL changes that then routes to NGINX team.
	‚Ä¢	Store NGINX config in Git; treat it like code (PR review, approvals, audit trail).

‚∏ª

3. Routing & Redirection Rules Explosion (Now in NGINX)

You‚Äôre solving ‚ÄúImperva rule bloat‚Äù‚Ä¶ by moving those rules to a proxy.

Challenges:
	‚Ä¢	NGINX config can itself become huge and hard to reason about:
	‚Ä¢	Thousands of redirects in map blocks.
	‚Ä¢	Many location blocks for different apps / regions / products.
	‚Ä¢	Risk of conflicting rules:
	‚Ä¢	A location / catch-all overshadowing more specific locations.
	‚Ä¢	Overlapping redirect maps and rewrite rules.

Mitigation ideas:
	‚Ä¢	Use separate include files per domain / application:
	‚Ä¢	e.g., /etc/nginx/conf.d/sc.com/redirects.conf, aem_routes.conf.
	‚Ä¢	For large redirect sets, consider:
	‚Ä¢	NGINX Plus key-value store; or
	‚Ä¢	Load redirects from a simple data file that CI generates.
	‚Ä¢	Add automated tests that:
	‚Ä¢	Hit critical URLs in a pipeline (curl/integration tests) before deploying NGINX config.

‚∏ª

4. Caching & Header Coordination (Imperva vs NGINX vs AEM/S3)

Now you have at least two cache layers (Imperva edge and maybe AEM Dispatcher/AEM/S3 headers).

Challenges:
	‚Ä¢	Wrong Cache-Control / Expires headers can create:
	‚Ä¢	Stale content at Imperva even after publishing in AEM.
	‚Ä¢	Redirects cached too long / too short.
	‚Ä¢	Different behavior for:
	‚Ä¢	HTML (short TTL or no cache).
	‚Ä¢	Static assets from S3 (long TTL, versioned paths).
	‚Ä¢	Cache invalidation becomes a multi-step process:
	‚Ä¢	AEM/Dispatcher flush ‚Üí Imperva purge ‚Üí (optionally) browser.

Mitigation ideas:
	‚Ä¢	Document a cache policy matrix:
‚ÄúFor HTML, TTL=X; for versioned static assets, TTL=Y; for 301s, TTL=Z.‚Äù
	‚Ä¢	Make sure:
	‚Ä¢	AEM/Dispatcher sets sane defaults.
	‚Ä¢	NGINX does not override them accidentally, unless explicitly.
	‚Ä¢	Automate Imperva cache invalidation in your release pipeline for static/CDN assets.

‚∏ª

5. SSL/TLS & Trust Chains

Challenges:
	‚Ä¢	External TLS termination at Imperva.
	‚Ä¢	Internal TLS from Imperva ‚Üí NGINX (optional but recommended).
	‚Ä¢	Maybe TLS again NGINX ‚Üí backend (APIs, AEM, etc).

This means:
	‚Ä¢	Internal certificates must be deployed on NGINX and backends.
	‚Ä¢	Imperva must trust internal CA (if you validate backend certs).
	‚Ä¢	Renewal processes for multiple certs.

Mitigation ideas:
	‚Ä¢	Use a standardized internal PKI or ACME-based automation for internal certs.
	‚Ä¢	Write a simple matrix:
	‚Ä¢	Public cert = managed via Security/Imperva.
	‚Ä¢	Internal certs = managed via Platform / DevOps.
	‚Ä¢	Monitor certificate expiry centrally.

‚∏ª

6. Performance & Latency Impact

Challenges:
	‚Ä¢	Extra hop (Imperva ‚Üí NGINX).
	‚Ä¢	If NGINX is not sized/tuned correctly:
	‚Ä¢	Connection saturation.
	‚Ä¢	Increased tail latency (P95 / P99).
	‚Ä¢	Risk of double compression, extra buffering, or slow backend timeouts.

Mitigation ideas:
	‚Ä¢	Performance testing:
	‚Ä¢	End-to-end load test with Imperva + NGINX + AEM.
	‚Ä¢	Tunings:
	‚Ä¢	Keepalive connections between Imperva ‚Üî NGINX and NGINX ‚Üî backend.
	‚Ä¢	Reasonable proxy_read_timeout and connection limits to avoid resource exhaustion.
	‚Ä¢	Ensure NGINX has:
	‚Ä¢	Enough CPU/RAM.
	‚Ä¢	Adequate ulimits (file descriptors, worker connections).

‚∏ª

7. Single Points of Failure & HA

Challenges:
	‚Ä¢	NGINX becomes a critical choke point (if it goes down, sc.com goes down).
	‚Ä¢	Misconfigured HA (one instance, no VIP, no failover) is risky.
	‚Ä¢	DR region might not have a symmetrical setup.

Mitigation ideas:
	‚Ä¢	Always at least two NGINX instances behind an internal load balancer or VIP.
	‚Ä¢	Health checks from:
	‚Ä¢	Imperva to NGINX VIP.
	‚Ä¢	NGINX to each backend (AEM, legacy, API).
	‚Ä¢	DR:
	‚Ä¢	Plan for a second NGINX cluster and AEM/legacy presence in DR environment.
	‚Ä¢	Pre-configured Imperva DR origin that can be activated via routing change.

‚∏ª

8. Logging, Monitoring & Troubleshooting

Multiple layers = multiple log sources.

Challenges:
	‚Ä¢	Correlating logs between:
	‚Ä¢	Imperva ‚Üí NGINX ‚Üí AEM ‚Üí APIs.
	‚Ä¢	Potentially different formats, timezones, and fields:
	‚Ä¢	Harder to reconstruct a user‚Äôs journey.

Mitigation ideas:
	‚Ä¢	Standardize on a correlation ID:
	‚Ä¢	Imperva injects X-Request-ID.
	‚Ä¢	NGINX logs it in access.log.
	‚Ä¢	AEM & backend frameworks also log it.
	‚Ä¢	Centralize logs into:
	‚Ä¢	Splunk / ELK / Datadog / SIEM.
	‚Ä¢	Build dashboards for:
	‚Ä¢	4xx/5xx by path.
	‚Ä¢	Cache HIT/MISS rates at Imperva.
	‚Ä¢	Latency breakdown (Imperva ‚Üí NGINX ‚Üí backend).

‚∏ª

9. Security Gaps or Overlaps

Challenges:
	‚Ä¢	Risk of assuming ‚ÄúImperva will block everything‚Äù and forgetting about:
	‚Ä¢	Internal-only paths exposed by mistake.
	‚Ä¢	Direct access to NGINX or backends bypassing Imperva (misconfigured firewall).
	‚Ä¢	Overlap between NGINX and Imperva security controls:
	‚Ä¢	Duplicate IP blocking or rate limiting in both places.

Mitigation ideas:
	‚Ä¢	Network controls:
	‚Ä¢	Only Imperva IPs can reach NGINX externally.
	‚Ä¢	Only NGINX can reach AEM/legacy APIs on those ports.
	‚Ä¢	Clear design:
	‚Ä¢	WAF rules: only on Imperva.
	‚Ä¢	Basic HTTP hardening & header security: on NGINX (e.g., X-Frame-Options, CSP).
	‚Ä¢	Periodic penetration tests that:
	‚Ä¢	Try accessing NGINX or AEM directly.
	‚Ä¢	Validate WAF + network controls.

‚∏ª

10. Migration / Coexistence with Old Rules

Challenges:
	‚Ä¢	During migration:
	‚Ä¢	Some redirects may still live in Imperva.
	‚Ä¢	New ones in NGINX ‚Üí potential conflicts or odd behavior.
	‚Ä¢	Risk of double redirect loops if both layers have rules for the same path.

Mitigation ideas:
	‚Ä¢	Plan migration in phases:
	1.	Inventory all Imperva routing/redirect rules.
	2.	Implement equivalent rules in NGINX.
	3.	Switch Imperva to a simple ‚Äúsend everything to NGINX‚Äù model for www.sc.com.
	4.	Clean up Imperva rules after validation.
	‚Ä¢	Have a test matrix for important URLs to validate behavior pre/post cutover.

‚∏ª

11. Bank-Specific Governance / Compliance

Especially for a bank like sc.com:

Challenges:
	‚Ä¢	Multiple regulators and internal audit checks:
	‚Ä¢	Need clear traceability of changes.
	‚Ä¢	Different teams across geographies:
	‚Ä¢	Region-specific URLs and rules (e.g., sc.com/in, sc.com/hk etc.).
	‚Ä¢	Need to ensure:
	‚Ä¢	Data residency rules aren‚Äôt violated (where AEM/NGINX/Imperva/S3 are hosted).

Mitigation ideas:
	‚Ä¢	Treat NGINX config as regulated code:
	‚Ä¢	PR approvals, peer review, change records.
	‚Ä¢	Use config templates for region-specific sites.
	‚Ä¢	Document data flow and hosting locations clearly for audit.

‚∏ª

If you‚Äôd like, I can turn this into a formal ‚ÄúRisks & Mitigations‚Äù section you can append directly to your technical design document for sc.com, with a severity (High/Med/Low) and owner for each risk.

Nice, this is exactly the messy real-world part everyone underestimates üòÑ
Let‚Äôs design it so WordPress and AEM can coexist cleanly while you‚Äôre mid-migration.

I‚Äôll assume this stack:
	‚Ä¢	Imperva (WAF + CDN) ‚Üí NGINX proxy ‚Üí WordPress + AEM
	‚Ä¢	Old rules currently point to WordPress
	‚Ä¢	Only some pages are migrated to AEM (not full site yet)

‚∏ª

1. Goal in this Mixed WP + AEM Phase

You need redirection/routing that:
	1.	Still serves non-migrated URLs from WordPress
	2.	Serves migrated URLs from AEM (even if the URL is the same)
	3.	For changed URLs, 301 redirect old WordPress URL ‚Üí new AEM URL
	4.	Avoids dumping a ton of new URL logic into Imperva (keep that in NGINX).

So the source of truth for ‚Äúwhere this URL goes‚Äù must move from Imperva to the proxy (NGINX).

‚∏ª

2. Conceptual Design

2.1 URL Categories

Every old WordPress URL will fall into one of these:
	1.	Unchanged URL, but now served from AEM
	‚Ä¢	/personal/cards (was WP, same URL now in AEM)
	2.	Changed URL, migrated to AEM
	‚Ä¢	Old: /personalbanking/cards (WordPress)
	‚Ä¢	New: /personal/cards (AEM)
‚Üí needs 301.
	3.	Not migrated (still on WordPress)
	‚Ä¢	/blog/2022/old-article/ stays on WP for now.
	4.	Retired
	‚Ä¢	Old URL should 301 to a replacement or 410/404.

You want a migration catalog like:

Old URL
New URL / Target
Target app
Action
/personalbanking/cards
/personal/cards
AEM
301
/personal/cards
/personal/cards
AEM
direct
/blog/2022/old-article/
(unchanged)
WP
direct
/product/legacy/feature-x
/product/new/feature-x
AEM
301


3. Where Logic Lives

Imperva
	‚Ä¢	Keeps:
	‚Ä¢	WAF, DDoS, TLS
	‚Ä¢	Basic origin mapping (www.sc.com ‚Üí NGINX)
	‚Ä¢	NO per-URL migration logic (except temporary during cutover)

NGINX / NGINX Plus
	‚Ä¢	Becomes routing brain:
	‚Ä¢	Knows, per URL, whether to:
	‚Ä¢	Issue a 301 to an AEM URL, or
	‚Ä¢	Serve directly from AEM, or
	‚Ä¢	Serve from WordPress.

‚∏ª

4. Implementation Pattern in NGINX

4.1 Simple Pattern: Map for Redirects + Map for Target App

For small/medium number of URLs, you can manage everything in flat NGINX maps.

How this behaves:
	‚Ä¢	/personalbanking/cards
‚Üí hits redirect map ‚Üí 301 /personal/cards (AEM URL, permanent for SEO).
	‚Ä¢	/personal/cards
‚Üí no redirect entry ‚Üí map returns target_app = aem ‚Üí proxied to AEM.
	‚Ä¢	/blog/2022/old-article/
‚Üí no redirect entry, no app override ‚Üí target_app = wp (default) ‚Üí WordPress.

You can split maps into separate include files for maintainability:
	‚Ä¢	maps_redirects_sc.com.conf
	‚Ä¢	maps_target_app_sc.com.conf

‚∏ª

4.2 Bigger Sites: External Data or NGINX Plus

If you have hundreds or thousands of migrated pages:
	‚Ä¢	Use NGINX Plus key-value store:
	‚Ä¢	Populate from a JSON/CSV via API.
	‚Ä¢	No reload needed for every migration change.
	‚Ä¢	Or generate static map files from your migration catalog automatically in CI/CD.

‚∏ª

5. Handling Old Imperva Rules Pointing to WordPress

Right now you probably have Imperva rules like:
	‚Ä¢	/personalbanking/cards ‚Üí origin: WordPress
	‚Ä¢	Some rewrites/redirects baked into Imperva.

The migration approach:
	1.	Freeze Imperva app logic for www.sc.com
	‚Ä¢	Stop adding new per-URL rules.
	2.	Mirror necessary redirects/routing into NGINX
	‚Ä¢	For pages now on AEM, add them to the redirect map / target_app map.
	3.	Change Imperva origin
	‚Ä¢	www.sc.com ‚Üí NGINX VIP, not directly to WordPress.
	4.	Gradually remove/disable old Imperva WordPress routing rules
	‚Ä¢	Once tested, keep Imperva rules minimal.

From outside, www.sc.com still ‚Äújust works‚Äù, but inside:
	‚Ä¢	NGINX decides AEM vs WordPress + redirects based on up-to-date migration map.

‚∏ª

6. Edge Cases & Gotchas

6.1 Same URL Exists in Both WP and AEM

E.g. /personal/cards exists on both platforms during transition.
	‚Ä¢	Decide who is the canonical:
	‚Ä¢	If AEM is the future: target_app ‚Üí aem.
	‚Ä¢	WP version should be hidden (only accessible internally).
	‚Ä¢	Optional: protect WP with basic auth or IP restrictions during migration.

6.2 SEO & Analytics
	‚Ä¢	Always use 301 (permanent) when moving from WP ‚Üí AEM for migrated URLs.
	‚Ä¢	Keep tracking IDs working:
	‚Ä¢	Ensure JS tracking still loads on AEM pages.
	‚Ä¢	Update any hard-coded URLs in WordPress templates to point to the new path.

6.3 Content Editors During Migration
	‚Ä¢	Decide editing freeze:
	‚Ä¢	For URLs being migrated to AEM, stop editing them in WordPress once migration starts.
	‚Ä¢	Or build sync/post-migration checklist:
	‚Ä¢	Check that AEM content matches final WP version before mapping the URL.

‚∏ª

7. How to Explain This in Your Doc / Meeting

You can describe it like this:

‚ÄúDuring the transition from WordPress to AEM, NGINX will act as the routing brain.
Imperva will send all www.sc.com traffic to NGINX.
NGINX uses a migration catalog to:
	‚Ä¢	301 old WordPress URLs to new AEM URLs,
	‚Ä¢	Serve migrated URLs directly from AEM, and
	‚Ä¢	Continue serving all non-migrated URLs from WordPress.
This allows us to migrate page-by-page without breaking existing URLs or cluttering Imperva‚Äôs rule base.‚Äù

‚∏ª

If you share a couple of real example URLs (old WP path ‚Üí new AEM path), I can write a ready-to-paste NGINX map + location config snippet using your exact patterns (e.g., /sg/en/, /in/en/, etc. for sc.com).








